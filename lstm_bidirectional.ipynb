{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import load_model\n",
    "\n",
    "import import_notebook\n",
    "from readWrite import ReadWrite\n",
    "from vectorToDoc import VectorToDoc\n",
    "from docToVector import DocToVector\n",
    "from dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line size more than 20, trim to: ['2356', ' 1257', ' 1689', ' 1391', ' 2330', ' 791', ' 80', ' 1060', ' 2002', ' 1884', ' 1060', ' 300', ' 2330', ' 1135', ' 1799', ' 2073', ' 1165', ' 2253', ' 185', ' 1207']\n",
      "line size more than 20, trim to: ['2356', ' 1257', ' 1239', ' 500', ' 851', ' 1568', ' 1932', ' 851', ' 1239', ' 2153', ' 138', ' 1239', ' 455', ' 1008', ' 1044', ' 21', ' 1243', ' 1302', ' 851', ' 1503']\n",
      "Read complete, total records: 1814\n",
      "Read complete, total records: 1814\n",
      "x_train: 1452, shape: (1452, 20)\n",
      "y_train: 1452, shape: (1452,)\n",
      "x_test: 362, shape: (362, 20)\n",
      "y_test: 362, shape: (362,)\n",
      "encoding_train shape: (1452, 3)\n",
      "encoding_test shape: (362, 3)\n"
     ]
    }
   ],
   "source": [
    "tweetFile = \"combineVectors.csv\"\n",
    "tweetClassFile = \"combineVectorsResult.txt\"\n",
    "\n",
    "readWrite = ReadWrite()\n",
    "xdata = readWrite.readFile(tweetFile)\n",
    "ydata = readWrite.readFileClassifier(tweetClassFile)\n",
    "\n",
    "x_train = xdata[:int(len(xdata) * 0.8) + 1]\n",
    "y_train = ydata[:int(len(ydata) * 0.8) + 1]\n",
    "x_test = xdata[int(len(xdata) * 0.8):-1]\n",
    "y_test = ydata[int(len(ydata) * 0.8):-1]\n",
    "\n",
    "print(\"x_train: {}, shape: {}\".format(len(x_train), x_train.shape))\n",
    "print(\"y_train: {}, shape: {}\".format(len(y_train), y_train.shape))\n",
    "print(\"x_test: {}, shape: {}\".format(len(x_test), x_test.shape))\n",
    "print(\"y_test: {}, shape: {}\".format(len(y_test), y_test.shape))\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(ydata)\n",
    "encoded = encoder.transform(ydata)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "encoding = np_utils.to_categorical(encoded)\n",
    "\n",
    "encoding_train = encoding[:int(len(xdata) * 0.8) + 1]\n",
    "encoding_test = encoding[int(len(ydata) * 0.8):-1]\n",
    "print(\"encoding_train shape: {}\".format(encoding_train.shape))\n",
    "print(\"encoding_test shape: {}\".format(encoding_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 20, 64)            192000    \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 217,027\n",
      "Trainable params: 217,027\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocabSize = 3000\n",
    "outputDim = 64\n",
    "recordsSize = 20\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, outputDim, input_length=recordsSize))\n",
    "model.add(Bidirectional(LSTM(32)))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1452 samples, validate on 362 samples\n",
      "Epoch 1/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0094 - acc: 0.9975 - val_loss: 1.2731 - val_acc: 0.8435\n",
      "Epoch 2/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0106 - acc: 0.9982 - val_loss: 1.4445 - val_acc: 0.8490\n",
      "Epoch 3/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0123 - acc: 0.9961 - val_loss: 1.3557 - val_acc: 0.8481\n",
      "Epoch 4/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0112 - acc: 0.9975 - val_loss: 1.5297 - val_acc: 0.8499\n",
      "Epoch 5/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0303 - acc: 0.9920 - val_loss: 1.2996 - val_acc: 0.8508\n",
      "Epoch 6/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0213 - acc: 0.9963 - val_loss: 1.0690 - val_acc: 0.8306\n",
      "Epoch 7/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0124 - acc: 0.9970 - val_loss: 1.3006 - val_acc: 0.8490\n",
      "Epoch 8/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0116 - acc: 0.9975 - val_loss: 1.3564 - val_acc: 0.8398\n",
      "Epoch 9/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0083 - acc: 0.9979 - val_loss: 1.3869 - val_acc: 0.8407\n",
      "Epoch 10/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0082 - acc: 0.9979 - val_loss: 1.4286 - val_acc: 0.8416\n",
      "Epoch 11/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0074 - acc: 0.9979 - val_loss: 1.4638 - val_acc: 0.8352\n",
      "Epoch 12/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0074 - acc: 0.9991 - val_loss: 1.4557 - val_acc: 0.8471\n",
      "Epoch 13/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0061 - acc: 0.9986 - val_loss: 1.5633 - val_acc: 0.8324\n",
      "Epoch 14/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0062 - acc: 0.9979 - val_loss: 1.5140 - val_acc: 0.8379\n",
      "Epoch 15/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0059 - acc: 0.9991 - val_loss: 1.5703 - val_acc: 0.8389\n",
      "Epoch 16/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0067 - acc: 0.9986 - val_loss: 1.5627 - val_acc: 0.8462\n",
      "Epoch 17/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0061 - acc: 0.9989 - val_loss: 1.5855 - val_acc: 0.8416\n",
      "Epoch 18/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0045 - acc: 0.9991 - val_loss: 1.6288 - val_acc: 0.8370\n",
      "Epoch 19/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0057 - acc: 0.9982 - val_loss: 1.6270 - val_acc: 0.8407\n",
      "Epoch 20/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0047 - acc: 0.9991 - val_loss: 1.6439 - val_acc: 0.8499\n",
      "Epoch 21/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0062 - acc: 0.9986 - val_loss: 1.7204 - val_acc: 0.8425\n",
      "Epoch 22/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0045 - acc: 0.9986 - val_loss: 1.6304 - val_acc: 0.8462\n",
      "Epoch 23/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0072 - acc: 0.9966 - val_loss: 1.6832 - val_acc: 0.8260\n",
      "Epoch 24/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0237 - acc: 0.9945 - val_loss: 1.2214 - val_acc: 0.8471\n",
      "Epoch 25/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0202 - acc: 0.9945 - val_loss: 1.4793 - val_acc: 0.8324\n",
      "Epoch 26/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0139 - acc: 0.9970 - val_loss: 1.5207 - val_acc: 0.8361\n",
      "Epoch 27/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0062 - acc: 0.9979 - val_loss: 1.5958 - val_acc: 0.8333\n",
      "Epoch 28/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0102 - acc: 0.9982 - val_loss: 1.6188 - val_acc: 0.8361\n",
      "Epoch 29/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0047 - acc: 0.9979 - val_loss: 1.6671 - val_acc: 0.8407\n",
      "Epoch 30/30\n",
      "1452/1452 [==============================] - 4s - loss: 0.0037 - acc: 0.9991 - val_loss: 1.6975 - val_acc: 0.8435\n",
      "Accuracy: 84.35%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 30\n",
    "\n",
    "# while(True):        \n",
    "#     model.fit(x_train, encoding_train, validation_data=(x_test, encoding_test), epochs=epochs, batch_size=outputDim, verbose=2)\n",
    "#     scores = model.evaluate(x_test, encoding_test, verbose=0)    \n",
    "#     print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "#     if(scores[1] > 0.85):\n",
    "#         break\n",
    "#     else:\n",
    "#         clear_output()\n",
    "\n",
    "model.fit(x_train, encoding_train, epochs=epochs, \n",
    "          batch_size=batch_size, validation_data=[x_test, encoding_test])\n",
    "scores = model.evaluate(x_test, encoding_test, verbose=0)    \n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.35%\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Saving model or load model\n",
    "'''\n",
    "model.save('model/bidir_model_Laptop.h5')\n",
    "# model = load_model('model/bidir_model_Laptop.h5')\n",
    "\n",
    "scores = model.evaluate(x_test, encoding_test, verbose=0)    \n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary count: 2307\n",
      "Dictionary count: 2305\n",
      "[['ccl', 'updat', 'pleas', 'add', 'min', 'addit', 'travel', 'time', 'payalebar', 'buonavista', 'due', 'train', 'fault', 'free', 'regular', 'bu', 'avail']]\n",
      "Converting records to vectors...\n",
      "Max length of record: 20\n",
      "IsPadding enable: True\n",
      "[[1795 1052  717  792 2048 1696 2001  486 1204 1341 1027 1069 2140  206\n",
      "  1385 1521  767    0    0    0]]\n",
      "[['ccl', 'updat', 'pleas', 'add', 'min', 'addit', 'travel', 'time', 'payalebar', 'buonavista', 'due', 'train', 'fault', 'free', 'regular', 'bu', 'avail', 'NONE', 'NONE', 'NONE']]\n",
      "1/1 [==============================] - 0s\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Classify sentence\n",
    "0-unknown, 1-update, 2-delay\n",
    "'''\n",
    "def classify_sentence(sentence, debug=False):\n",
    "    dic = Dictionary()\n",
    "    vectorToDoc = VectorToDoc()\n",
    "    docToVector = DocToVector()\n",
    "    senArr = dic.readSentence(sentence, True)\n",
    "    if(debug):\n",
    "        print(senArr)\n",
    "\n",
    "    vector = docToVector.convertVector(senArr, True)\n",
    "    sen = vectorToDoc.convertDoc(vector)\n",
    "    vector = np.array(vector)\n",
    "    if(debug):\n",
    "        print(vector)\n",
    "        print(sen)\n",
    "\n",
    "    predicted = model.predict_classes(vector)\n",
    "    print(predicted[0])\n",
    "    \n",
    "classify_sentence(\"[CCL Update] please add 30mins additional travelling time from #PayaLebar to #BuonaVista due to train fault. Free Regular bus available.\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary count: 2305\n",
      "Read complete, total records: 689\n",
      "vector records shape: (689, 20)\n",
      "672/689 [============================>.] - ETA: 0s\n",
      "(689,)\n",
      "1\n",
      "@SMRT_Singapore Rubbish. I am on train now. The announcement and train still indicate it will terminate at Ang Mo Kio.\n",
      "\n",
      "1\n",
      "@SBSTransit_Ltd what time is the first train for the downtown line at bukit panjang on the sunday?  reply by tdy please\n",
      "\n",
      "1\n",
      "@SMRT_Singapore honestly public transport is supposed to make life more convenient but that ain't the case with y'all.\n",
      "\n",
      "1\n",
      "@SMRT_Singapore Can that someone commit hara kiri?\n",
      "\n",
      "1\n",
      "@SMRT_Singapore u bloody hell better pay me back my money for cab to town from Tpy leh legit fucked up yall\n",
      "\n",
      "1\n",
      "@SMRT_Singapore #mrt is down again and there is mess in AMK station. No officers at the platform to guide commuters.\n",
      "\n",
      "1\n",
      "@SMRT_Singapore On train and yet NOT a single announcement...well done again..\n",
      "\n",
      "1\n",
      "@SMRT_Singapore Really perfect. Just boarded to train. :(\n",
      "\n",
      "1\n",
      "@SBSTransit_Ltd #DTL  I think Chinatwn stat.. should block off Platform D entry w visible notice there when it closed .. don't be lazy\n",
      "\n",
      "1\n",
      "@SMRT_Singapore Thank you for the information.\n",
      "\n",
      "1\n",
      "@SMRT_Singapore there seems to be a lost child on NEL, at Potong Pasir now. Towards Punggol.\n",
      "\n",
      "1\n",
      "@SBSTransit_Ltd Smells like someone took a dump on bus 143, SBS 3421A!\n",
      "\n",
      "1\n",
      "@SBSTransit_Ltd why do your buses drive so slow wtf\n",
      "\n",
      "1\n",
      "@SMRT_Singapore given up on updates? What happened this morning?\n",
      "\n",
      "1\n",
      "@SMRT_Singapore Are u sure there's no train fault now?? How come no announcement? Been at the platform for 30mins yet can't board the train!\n",
      "\n",
      "1\n",
      "@SMRT_Singapore idea of a world class transport! Allowing platforms to overflow to dangerous levels with train delaâ€¦ https://t.co/1U71AenTTW\n",
      "\n",
      "1\n",
      "@SMRT_Singapore NSL train fault?! Train stop at TPY for long.\n",
      "\n",
      "1\n",
      "@SMRT_Singapore Admiralty to Newton take hour. What the hell?! What have u done. Repeated every week.\n",
      "\n",
      "1\n",
      "@SBSTransit_Ltd  Is there any way to load the pdf file? https://t.co/1xMfHheURG\n",
      "\n",
      "1\n",
      "@SBSTransit_Ltd Why must be ceased because it meet passengger dificuted for the comunter.\n",
      "\n",
      "1\n",
      "@SBSTransit_Ltd Can faster? I late for test already. Bus 188\n",
      "\n",
      "1\n",
      "@SBSTransit_Ltd OR DO I HAVE TO GO OUT 3-4HRS BEFORE??????????ðŸ˜’ðŸ˜’ðŸ˜’ðŸ˜’ðŸ˜’\n",
      "\n",
      "1\n",
      "@SBSTransit_Ltd PLEASE STEP UP YOUR GAME THANKS IM HERE FOR THE PAST 20 minutes WHERE MY BUS AT\n",
      "\n",
      "1\n",
      "@SMRT_Singapore Shithead can still go on parliament say things are better now. What kind of fking retard does that?\n",
      "\n",
      "1\n",
      "@SBSTransit_Ltd @STcom @TeresaSherylA proof see\n",
      "\n",
      "1\n",
      "@SMRT_Singapore Show the times train breakdown too\n",
      "\n",
      "1\n",
      "@SMRT_Singapore What a day on 9/11.  !! Trains down and PE is joke of the century.\n",
      "\n",
      "1\n",
      "@SMRT_Singapore Make use of this account to update things like overcrowding so we can seek alternative transport BEFORE we reach the station\n",
      "\n",
      "1\n",
      "@SBSTransit_Ltd DO YALL EVER CLEAN YOUR BUSES???? I'VE SEEN AT LEAST 5 TINY COCKROACHES ON 4 DIFFERENT BUSES\n",
      "\n",
      "1\n",
      "@SMRT_Singapore, is the train that bad? Once i boarded the train then announced train fault??\n",
      "\n",
      "1\n",
      "@SMRT_Singapore Picked up 3 tourist passes at Farrer park MRT and was provided bad information about validity, really feel let down.\n",
      "\n",
      "1\n",
      "@SBSTransit_Ltd why does bus 186 have to be late zzz\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "0-unknown, 1-update, 2-delay\n",
    "'''\n",
    "dic = Dictionary()\n",
    "docToVector = DocToVector()\n",
    "\n",
    "# testArr = dic.readFile(\"user_tweets4.txt\", True)\n",
    "# vectorsArr2 = docToVector.convertVector(testArr, True)\n",
    "# vectorsArr2 = np.array(vectorsArr2)\n",
    "# oriRecords2 = readWrite.readOriFile('user_tweets4.txt')\n",
    "\n",
    "vectorsArr2 = readWrite.readFile('docVectors3.csv')\n",
    "oriRecords2 = readWrite.readOriFile('user_tweets3.txt')\n",
    "\n",
    "print(\"vector records shape: {}\".format(vectorsArr2.shape))\n",
    "\n",
    "classResult = []\n",
    "predicted = model.predict_classes(vectorsArr2)\n",
    "predicted = np.reshape(predicted, (predicted.size,))\n",
    "\n",
    "print()\n",
    "print(predicted.shape)\n",
    "\n",
    "for idx, score in enumerate(predicted):    \n",
    "    if(score == 1): \n",
    "        classResult.append(oriRecords2[idx])       \n",
    "        print(predicted[idx])\n",
    "        print(oriRecords2[idx])        \n",
    "\n",
    "# readWrite.writeFile('docVectors3Result.txt', predicted)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
