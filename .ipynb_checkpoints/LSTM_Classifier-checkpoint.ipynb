{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from readWrite.ipynb\n",
      "importing Jupyter notebook from vectorToDoc.ipynb\n",
      "importing Jupyter notebook from dictionary.ipynb\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\himur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\himur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "importing Jupyter notebook from docToVector.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import import_notebook\n",
    "from readWrite import ReadWrite\n",
    "from vectorToDoc import VectorToDoc\n",
    "from docToVector import DocToVector\n",
    "from dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read complete, total records: 1814\n",
      "Read complete, total records: 1814\n",
      "x_train: 1452, shape: (1452, 20)\n",
      "y_train: 1452, shape: (1452,)\n",
      "x_test: 362, shape: (362, 20)\n",
      "y_test: 362, shape: (362,)\n",
      "encoding_train shape: (1452, 3)\n",
      "encoding_test shape: (362, 3)\n"
     ]
    }
   ],
   "source": [
    "tweetFile = \"combineVectors.csv\"\n",
    "tweetClassFile = \"combineVectorsResult.txt\"\n",
    "\n",
    "readWrite = ReadWrite()\n",
    "xdata = readWrite.readFile(tweetFile)\n",
    "ydata = readWrite.readFileClassifier(tweetClassFile)\n",
    "\n",
    "x_train = xdata[:int(len(xdata) * 0.8) + 1]\n",
    "y_train = ydata[:int(len(ydata) * 0.8) + 1]\n",
    "x_test = xdata[int(len(xdata) * 0.8):-1]\n",
    "y_test = ydata[int(len(ydata) * 0.8):-1]\n",
    "\n",
    "print(\"x_train: {}, shape: {}\".format(len(x_train), x_train.shape))\n",
    "print(\"y_train: {}, shape: {}\".format(len(y_train), y_train.shape))\n",
    "print(\"x_test: {}, shape: {}\".format(len(x_test), x_test.shape))\n",
    "print(\"y_test: {}, shape: {}\".format(len(y_test), y_test.shape))\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(ydata)\n",
    "encoded = encoder.transform(ydata)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "encoding = np_utils.to_categorical(encoded)\n",
    "\n",
    "encoding_train = encoding[:int(len(xdata) * 0.8) + 1]\n",
    "encoding_test = encoding[int(len(ydata) * 0.8):-1]\n",
    "print(\"encoding_train shape: {}\".format(encoding_train.shape))\n",
    "print(\"encoding_test shape: {}\".format(encoding_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 20, 64)            128000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 194,303\n",
      "Trainable params: 194,303\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocabSize = 2000\n",
    "outputDim = 64\n",
    "recordsSize = 20\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, outputDim, input_length=recordsSize))\n",
    "# model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, input_dim=(20,), activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1452 samples, validate on 362 samples\n",
      "Epoch 1/25\n",
      "1s - loss: 0.0863 - acc: 0.9713 - val_loss: 0.9237 - val_acc: 0.8260\n",
      "Epoch 2/25\n",
      "1s - loss: 0.0780 - acc: 0.9743 - val_loss: 0.8248 - val_acc: 0.8057\n",
      "Epoch 3/25\n",
      "1s - loss: 0.0755 - acc: 0.9754 - val_loss: 0.8408 - val_acc: 0.8076\n",
      "Epoch 4/25\n",
      "1s - loss: 0.0621 - acc: 0.9812 - val_loss: 0.9887 - val_acc: 0.8177\n",
      "Epoch 5/25\n",
      "1s - loss: 0.0578 - acc: 0.9826 - val_loss: 0.9741 - val_acc: 0.8149\n",
      "Epoch 6/25\n",
      "1s - loss: 0.0571 - acc: 0.9800 - val_loss: 0.9668 - val_acc: 0.8168\n",
      "Epoch 7/25\n",
      "2s - loss: 0.0446 - acc: 0.9860 - val_loss: 0.8320 - val_acc: 0.8223\n",
      "Epoch 8/25\n",
      "1s - loss: 0.0384 - acc: 0.9892 - val_loss: 1.1455 - val_acc: 0.8241\n",
      "Epoch 9/25\n",
      "1s - loss: 0.0383 - acc: 0.9876 - val_loss: 1.1084 - val_acc: 0.7947\n",
      "Epoch 10/25\n",
      "1s - loss: 0.0456 - acc: 0.9855 - val_loss: 0.8624 - val_acc: 0.8195\n",
      "Epoch 11/25\n",
      "1s - loss: 0.0293 - acc: 0.9894 - val_loss: 1.2440 - val_acc: 0.8223\n",
      "Epoch 12/25\n",
      "1s - loss: 0.0287 - acc: 0.9908 - val_loss: 1.1141 - val_acc: 0.8306\n",
      "Epoch 13/25\n",
      "1s - loss: 0.0279 - acc: 0.9910 - val_loss: 1.2632 - val_acc: 0.8103\n",
      "Epoch 14/25\n",
      "1s - loss: 0.0295 - acc: 0.9906 - val_loss: 1.1018 - val_acc: 0.8103\n",
      "Epoch 15/25\n",
      "1s - loss: 0.0236 - acc: 0.9915 - val_loss: 1.0984 - val_acc: 0.8066\n",
      "Epoch 16/25\n",
      "1s - loss: 0.0212 - acc: 0.9936 - val_loss: 1.1777 - val_acc: 0.7983\n",
      "Epoch 17/25\n",
      "1s - loss: 0.0186 - acc: 0.9947 - val_loss: 1.2853 - val_acc: 0.8085\n",
      "Epoch 18/25\n",
      "1s - loss: 0.0163 - acc: 0.9949 - val_loss: 1.3263 - val_acc: 0.8020\n",
      "Epoch 19/25\n",
      "1s - loss: 0.0164 - acc: 0.9954 - val_loss: 1.3406 - val_acc: 0.7956\n",
      "Epoch 20/25\n",
      "1s - loss: 0.0158 - acc: 0.9945 - val_loss: 1.3227 - val_acc: 0.8122\n",
      "Epoch 21/25\n",
      "1s - loss: 0.0167 - acc: 0.9949 - val_loss: 1.2304 - val_acc: 0.8131\n",
      "Epoch 22/25\n",
      "1s - loss: 0.0117 - acc: 0.9959 - val_loss: 1.3243 - val_acc: 0.8048\n",
      "Epoch 23/25\n",
      "1s - loss: 0.0236 - acc: 0.9927 - val_loss: 1.1942 - val_acc: 0.8112\n",
      "Epoch 24/25\n",
      "1s - loss: 0.0125 - acc: 0.9956 - val_loss: 1.4491 - val_acc: 0.8103\n",
      "Epoch 25/25\n",
      "1s - loss: 0.0114 - acc: 0.9961 - val_loss: 1.5033 - val_acc: 0.8131\n",
      "Accuracy: 81.31%\n"
     ]
    }
   ],
   "source": [
    "batch_size = outputDim\n",
    "epochs = 25\n",
    "scores = 0\n",
    "\n",
    "# while(True):        \n",
    "#     model.fit(x_train, encoding_train, validation_data=(x_test, encoding_test), epochs=epochs, batch_size=outputDim, verbose=2)\n",
    "#     scores = model.evaluate(x_test, encoding_test, verbose=0)    \n",
    "#     print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "#     if(scores[1] > 0.83):\n",
    "#         break\n",
    "#     else:\n",
    "#         clear_output()\n",
    "\n",
    "model.fit(x_train, encoding_train, validation_data=(x_test, encoding_test), epochs=epochs, batch_size=outputDim, verbose=2)\n",
    "scores = model.evaluate(x_test, encoding_test, verbose=0)    \n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Saving model or load model\n",
    "'''\n",
    "# model.save('model/tweetClassifierLaptop.h5')\n",
    "# model = load_model('model/tweetClassifier1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary count: 2377\n",
      "Dictionary count: 2375\n",
      "[['smrt', 'singapor', 'minut', 'travel', 'time', 'lakesid', 'pioneer', 'u', 'start', 'test']]\n",
      "Converting records to vectors...\n",
      "Max length of record: 20\n",
      "IsPadding enable: True\n",
      "[[1293 1692  939  672 1802  313 1794 2313  457 2035    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n",
      "[['smrt', 'singapor', 'minut', 'travel', 'time', 'lakesid', 'pioneer', 'u', 'start', 'test', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE']]\n",
      "1/1 [==============================] - 0s\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Classify sentence\n",
    "0-unknown, 1-update, 2-delay\n",
    "'''\n",
    "dic = Dictionary()\n",
    "vectorToDoc = VectorToDoc()\n",
    "docToVector = DocToVector()\n",
    "\n",
    "sentence = \"NSL Woodlands train signal fault please take alternative transport\"\n",
    "senArr = dic.readSentence(sentence, True)\n",
    "print(senArr)\n",
    "\n",
    "vector = docToVector.convertVector(senArr, True)\n",
    "sen = vectorToDoc.convertDoc(vector)\n",
    "vector = np.array(vector)\n",
    "print(vector)\n",
    "print(sen)\n",
    "\n",
    "predicted = model.predict_classes(vector)\n",
    "print(predicted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting records to vectors...\n",
      "Max length of record: 20\n",
      "IsPadding enable: True\n",
      "Shape: (3, 20)\n",
      "3/3 [==============================] - 0s\n",
      "\n",
      "(3,)\n",
      "1\n",
      "It must be stressful day for SMRT staff today, let me know if you need volunteers for crowd control… https://t.co/nxuVrloVYv\n",
      "\n",
      "2\n",
      "NSL.. When will this all train signalling fault end\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "0-unknown, 1-update, 2-delay\n",
    "'''\n",
    "testArr = dic.readFile(\"user_tweets4.txt\", True)\n",
    "vectorsArr2 = docToVector.convertVector(testArr, True)\n",
    "vectorsArr2 = np.array(vectorsArr2)\n",
    "oriRecords2 = readWrite.readOriFile('user_tweets4.txt')\n",
    "# print(vectorsArr2)\n",
    "\n",
    "# vectorsArr2 = readWrite.readFile('docVectors3.csv')\n",
    "# oriRecords2 = readWrite.readOriFile('user_tweets3.txt')\n",
    "print(\"Shape: {}\".format(vectorsArr2.shape))\n",
    "\n",
    "classResult = []\n",
    "predicted = model.predict_classes(vectorsArr2)\n",
    "predicted = np.reshape(predicted, (predicted.size,))\n",
    "\n",
    "print()\n",
    "print(predicted.shape)\n",
    "\n",
    "for idx, score in enumerate(predicted):    \n",
    "    if(score > 0): \n",
    "        classResult.append(oriRecords2[idx])       \n",
    "        print(predicted[idx])\n",
    "        print(oriRecords2[idx])        \n",
    "\n",
    "# readWrite.writeFile('docVectors3Result.txt', predicted)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
