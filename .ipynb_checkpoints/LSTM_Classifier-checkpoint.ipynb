{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from readWrite.ipynb\n",
      "importing Jupyter notebook from vectorToDoc.ipynb\n",
      "importing Jupyter notebook from dictionary.ipynb\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\himur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\himur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "importing Jupyter notebook from docToVector.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import import_notebook\n",
    "from readWrite import ReadWrite\n",
    "from vectorToDoc import VectorToDoc\n",
    "from docToVector import DocToVector\n",
    "from dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read complete, total records: 1814\n",
      "Read complete, total records: 1814\n",
      "x_train: 1452, shape: (1452, 20)\n",
      "y_train: 1452, shape: (1452,)\n",
      "x_test: 362, shape: (362, 20)\n",
      "y_test: 362, shape: (362,)\n",
      "encoding_train shape: (1452, 3)\n",
      "encoding_test shape: (362, 3)\n"
     ]
    }
   ],
   "source": [
    "tweetFile = \"combineVectors.csv\"\n",
    "tweetClassFile = \"combineVectorsResult.txt\"\n",
    "\n",
    "readWrite = ReadWrite()\n",
    "xdata = readWrite.readFile(tweetFile)\n",
    "ydata = readWrite.readFileClassifier(tweetClassFile)\n",
    "\n",
    "x_train = xdata[:int(len(xdata) * 0.8) + 1]\n",
    "y_train = ydata[:int(len(ydata) * 0.8) + 1]\n",
    "x_test = xdata[int(len(xdata) * 0.8):-1]\n",
    "y_test = ydata[int(len(ydata) * 0.8):-1]\n",
    "\n",
    "print(\"x_train: {}, shape: {}\".format(len(x_train), x_train.shape))\n",
    "print(\"y_train: {}, shape: {}\".format(len(y_train), y_train.shape))\n",
    "print(\"x_test: {}, shape: {}\".format(len(x_test), x_test.shape))\n",
    "print(\"y_test: {}, shape: {}\".format(len(y_test), y_test.shape))\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(ydata)\n",
    "encoded = encoder.transform(ydata)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "encoding = np_utils.to_categorical(encoded)\n",
    "\n",
    "encoding_train = encoding[:int(len(xdata) * 0.8) + 1]\n",
    "encoding_test = encoding[int(len(ydata) * 0.8):-1]\n",
    "print(\"encoding_train shape: {}\".format(encoding_train.shape))\n",
    "print(\"encoding_test shape: {}\".format(encoding_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 20, 64)            128000    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100)               66000     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 194,303\n",
      "Trainable params: 194,303\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocabSize = 2000\n",
    "outputDim = 64\n",
    "recordsSize = 30\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabSize, outputDim, input_length=recordsSize))\n",
    "# model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, input_dim=(20,), activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1452 samples, validate on 362 samples\n",
      "Epoch 1/10\n",
      "2s - loss: 0.4707 - acc: 0.7863 - val_loss: 0.4750 - val_acc: 0.7993\n",
      "Epoch 2/10\n",
      "1s - loss: 0.3632 - acc: 0.8641 - val_loss: 0.4635 - val_acc: 0.8020\n",
      "Epoch 3/10\n",
      "1s - loss: 0.2831 - acc: 0.8882 - val_loss: 0.4301 - val_acc: 0.8204\n",
      "Epoch 4/10\n",
      "1s - loss: 0.1982 - acc: 0.9123 - val_loss: 0.4505 - val_acc: 0.8223\n",
      "Epoch 5/10\n",
      "2s - loss: 0.1546 - acc: 0.9229 - val_loss: 0.6459 - val_acc: 0.8306\n",
      "Epoch 6/10\n",
      "1s - loss: 0.1376 - acc: 0.9261 - val_loss: 0.6409 - val_acc: 0.8260\n",
      "Epoch 7/10\n",
      "1s - loss: 0.1245 - acc: 0.9314 - val_loss: 0.8555 - val_acc: 0.8306\n",
      "Epoch 8/10\n",
      "1s - loss: 0.1049 - acc: 0.9529 - val_loss: 0.7605 - val_acc: 0.8306\n",
      "Epoch 9/10\n",
      "1s - loss: 0.0997 - acc: 0.9591 - val_loss: 0.9763 - val_acc: 0.8352\n",
      "Epoch 10/10\n",
      "1s - loss: 0.0844 - acc: 0.9676 - val_loss: 0.7839 - val_acc: 0.8011\n",
      "Accuracy: 80.11%\n"
     ]
    }
   ],
   "source": [
    "batch_size = outputDim\n",
    "epochs = 10\n",
    "scores = 0\n",
    "\n",
    "# while(True):        \n",
    "#     model.fit(x_train, encoding_train, validation_data=(x_test, encoding_test), epochs=epochs, batch_size=outputDim, verbose=2)\n",
    "#     scores = model.evaluate(x_test, encoding_test, verbose=0)    \n",
    "#     print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "#     if(scores[1] > 0.83):\n",
    "#         break\n",
    "#     else:\n",
    "#         clear_output()\n",
    "\n",
    "model.fit(x_train, encoding_train, validation_data=(x_test, encoding_test), epochs=epochs, batch_size=outputDim, verbose=2)\n",
    "scores = model.evaluate(x_test, encoding_test, verbose=0)    \n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Saving model or load model\n",
    "'''\n",
    "# model.save('model/tweetClassifierLaptop.h5')\n",
    "# model = load_model('model/tweetClassifier1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary count: 2377\n",
      "Dictionary count: 2375\n",
      "[['nsl', 'train', 'signal', 'fault', 'end']]\n",
      "Converting records to vectors...\n",
      "Max length of record: 20\n",
      "IsPadding enable: True\n",
      "[[1603 1111 2362 2055 1837    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0]]\n",
      "[['nsl', 'train', 'signal', 'fault', 'end', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE', 'NONE']]\n",
      "1/1 [==============================] - 0s\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Classify sentence\n",
    "0-unknown, 1-update, 2-delay\n",
    "'''\n",
    "dic = Dictionary()\n",
    "vectorToDoc = VectorToDoc()\n",
    "docToVector = DocToVector()\n",
    "\n",
    "sentence = \"NSL.. When will this all train signalling fault end\"\n",
    "senArr = dic.readSentence(sentence, True)\n",
    "print(senArr)\n",
    "\n",
    "vector = docToVector.convertVector(senArr, True)\n",
    "sen = vectorToDoc.convertDoc(vector)\n",
    "vector = np.array(vector)\n",
    "print(vector)\n",
    "print(sen)\n",
    "\n",
    "predicted = model.predict_classes(vector)\n",
    "print(predicted[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting records to vectors...\n",
      "Max length of record: 20\n",
      "IsPadding enable: True\n",
      "Shape: (62, 20)\n",
      "32/62 [==============>...............] - ETA: 0s\n",
      "(62,)\n",
      "2\n",
      "@SMRT_Singapore @STcom Wow I've been stuck in between bukit batok and jurong east for 30 minutes already. Thanks for making me miss my movie\n",
      "\n",
      "2\n",
      "@SMRT_Singapore @LTAsg How long is your NEW signalling system allowed to malfunction before you declare that it's shit?\n",
      "\n",
      "2\n",
      "[NSL]: Due to a train fault on the new signalling system, please add 20mins train travel time between #ChoaChuKang and #JurongEast.\n",
      "\n",
      "2\n",
      "@SMRT_Singapore sleep at amk wake up though reach novena BUT still not yet reach toapayoh u are the best\n",
      "\n",
      "2\n",
      "@SMRT_Singapore train fault again at #NSL. Stupid service. Train waiting time oled pass 5 minutes lah! please provide correct delay time. Fk!\n",
      "\n",
      "2\n",
      "@SMRT_Singapore Haha....kbw open mouth n bless dt3... expect train issue in the coming few days\n",
      "\n",
      "2\n",
      "@SMRT_Singapore (Not linked to new signalling project) &lt;-- like that matters.\n",
      "\n",
      "2\n",
      "[EWL] UPDATE: Train services are progressively being restored. please add 10mins train travel time between #TanahMerah and #PasirRis.\n",
      "\n",
      "2\n",
      "@SMRT_Singapore Not linked wo. Aren't you glad? ðŸ˜‚\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "0-unknown, 1-update, 2-delay\n",
    "'''\n",
    "testArr = dic.readFile(\"user_tweets4.txt\", True)\n",
    "vectorsArr2 = docToVector.convertVector(testArr, True)\n",
    "vectorsArr2 = np.array(vectorsArr2)\n",
    "oriRecords2 = readWrite.readOriFile('user_tweets4.txt')\n",
    "# print(vectorsArr2)\n",
    "\n",
    "# vectorsArr2 = readWrite.readFile('docVectors3.csv')\n",
    "# oriRecords2 = readWrite.readOriFile('user_tweets3.txt')\n",
    "print(\"Shape: {}\".format(vectorsArr2.shape))\n",
    "\n",
    "classResult = []\n",
    "predicted = model.predict_classes(vectorsArr2)\n",
    "predicted = np.reshape(predicted, (predicted.size,))\n",
    "\n",
    "print()\n",
    "print(predicted.shape)\n",
    "\n",
    "for idx, score in enumerate(predicted):    \n",
    "    if(score > 0): \n",
    "        classResult.append(oriRecords2[idx])       \n",
    "        print(predicted[idx])\n",
    "        print(oriRecords2[idx])        \n",
    "\n",
    "# readWrite.writeFile('docVectors3Result.txt', predicted)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
